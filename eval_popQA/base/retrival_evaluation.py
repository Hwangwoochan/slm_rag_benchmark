import time
import re
import json
import pickle
import csv
import gc
import os

import numpy as np
import faiss
from tqdm import tqdm
from datasets import load_dataset
from sentence_transformers import SentenceTransformer

# =================================================
# 1. ì„¤ì •
# =================================================
TARGET_CONFIGS = [
    ("VEC",  "T512_O128", 3),
    ("BM25", "T512_O128", 3),
    ("VEC",  "T256_O64",  3),
    ("BM25", "T256_O64",  3),
    ("VEC",  "T512_O128", 2),
    ("BM25", "T512_O128", 2),
    ("VEC",  "W100",      2),
    ("BM25", "T256_O64",  2),
    ("VEC",  "T256_O64",  2),
    ("BM25", "W100",      3),
]

EMBED_MODEL = "all-MiniLM-L6-v2"
DATA_DIR = "./data"
OUTPUT_CSV = "retrieval_benchmark_with_recall.csv"

# í‰ê°€ ë°ì´í„°(ì§ˆë¬¸/ì •ë‹µ) ì„¤ì •
DATASET_NAME = "akariasai/PopQA"
DATASET_SPLIT = "test"
SAMPLE_SIZE = 200          # ì§ˆë¬¸ ìˆ˜
SEED = 42                  # ìƒ˜í”Œë§ ê³ ì •(ì¬í˜„ì„±)

# =================================================
# 2. ì •ë‹µ íŒŒì‹±/ë¦¬ì½œ ê³„ì‚° ìœ í‹¸
# =================================================
def normalize(s: str) -> str:
    return re.sub(r"\W+", " ", str(s).lower()).strip()

def parse_answers(ans):
    """PopQA possible_answersëŠ” list/ì¤‘ì²©list/str(json) ë“± ë‹¤ì–‘í•  ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ í¼ì¹¨."""
    if ans is None:
        return []
    if isinstance(ans, str):
        try:
            ans = json.loads(ans)
        except Exception:
            return [normalize(ans)]

    flat = []
    for a in ans:
        if isinstance(a, list):
            flat.extend(a)
        else:
            flat.append(a)

    out = []
    for a in flat:
        a_n = normalize(a)
        if a_n:
            out.append(a_n)
    # ì¤‘ë³µ ì œê±°
    return list(set(out))

def hit_at_k_texts(texts, answers_norm):
    """Top-k retrieved í…ìŠ¤íŠ¸ë“¤ ì¤‘ í•˜ë‚˜ë¼ë„ ì •ë‹µ ë¬¸ìì—´ì„ í¬í•¨í•˜ë©´ 1 else 0"""
    if not answers_norm:
        return 0
    for t in texts:
        t_n = normalize(t)
        for a in answers_norm:
            # ë„ˆë¬´ ì§§ì€ ë‹µì€ ì˜¤íƒ ë§ì•„ì„œ ë¬´ì‹œ (ì›í•˜ë©´ ê¸°ì¤€ ë³€ê²½)
            if len(a) < 2:
                continue
            if a in t_n:
                return 1
    return 0

# =================================================
# 3. ë©”ì¸ ì‹¤í–‰
# =================================================
def run_retrieval_benchmarks():
    print(f"--- Retrieval ë²¤ì¹˜ë§ˆí¬(ì§ˆë¬¸ ìƒ˜í”Œë§ + recall@k ê³„ì‚°) ì‹œì‘ ---")

    # 1) í‰ê°€ ë°ì´í„°ì—ì„œ ì§ˆë¬¸/ì •ë‹µ ìƒ˜í”Œë§ (SEED ê³ ì •)
    ds = load_dataset(DATASET_NAME, split=DATASET_SPLIT)
    ds = ds.shuffle(seed=SEED).select(range(SAMPLE_SIZE))

    test_items = []
    for ex in ds:
        q = ex.get("question", "")
        answers = parse_answers(ex.get("possible_answers", []))
        if q and answers:
            test_items.append((q, answers))

    print(f"[INFO] Loaded {len(test_items)} Q/A items from {DATASET_NAME}:{DATASET_SPLIT}")

    # 2) ì„ë² ë”© ëª¨ë¸ 1íšŒ ë¡œë“œ
    print(f"[INFO] Loading embed model: {EMBED_MODEL}")
    embedder = SentenceTransformer(EMBED_MODEL)

    # 3) CSV í—¤ë”
    with open(OUTPUT_CSV, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow([
            "Retrieval_Type", "Chunk_Name", "Top_K",
            "Recall@K(hit@k)", "Avg_Time(s)", "Min_Time(s)", "Max_Time(s)"
        ])

    # 4) ì¡°í•© ë£¨í”„
    for ret_type, chunk, top_k in TARGET_CONFIGS:
        print(f"\nğŸ” [ì¸¡ì •] {ret_type} | {chunk} | K={top_k}")

        # ì´ì „ ë£¨í”„ ë©”ëª¨ë¦¬ ì •ë¦¬
        index = None
        data_obj = None
        gc.collect()

        # ì¸ë±ìŠ¤/ë©”íƒ€ ë¡œë“œ
        try:
            if ret_type == "VEC":
                idx_path = f"{DATA_DIR}/vector_indices/{chunk}/faiss.index"
                meta_path = f"{DATA_DIR}/vector_indices/{chunk}/metas.pkl"

                index = faiss.read_index(idx_path)
                with open(meta_path, "rb") as f:
                    data_obj = pickle.load(f)  # list[{"text":...}, ...] í˜•íƒœë¼ê³  ê°€ì •
            else:
                bm25_path = f"{DATA_DIR}/bm25_indices/{chunk}/bm25.pkl"
                with open(bm25_path, "rb") as f:
                    data_obj = pickle.load(f)
                    index = data_obj["bm25"]    # bm25 ê°ì²´
        except Exception as e:
            print(f"    >> ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue

        r_times = []
        hits = []

        # ì§ˆë¬¸ ë£¨í”„
        for q, answers_norm in tqdm(test_items, desc="      ì¸¡ì • ì¤‘", leave=False):
            start_r = time.perf_counter()

            if ret_type == "VEC":
                q_emb = embedder.encode([q], normalize_embeddings=True).astype("float32")
                _, idx = index.search(q_emb, top_k)

                # í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒˆë¡œ ë§Œë“¤ë˜(ë¦¬ì½œ ê³„ì‚° ìœ„í•´ í•„ìš”),
                # top_kë§Œí¼ë§Œ ë½‘ìœ¼ë‹ˆ ë¶€ë‹´ì´ ì ìŒ
                texts = []
                for i in idx[0]:
                    if i != -1:
                        texts.append(data_obj[i]["text"])
            else:
                scores = index.get_scores(q.split())
                idx = np.argsort(scores)[::-1][:top_k]
                texts = [data_obj["metas"][i]["text"] for i in idx]

            r_times.append(time.perf_counter() - start_r)
            hits.append(hit_at_k_texts(texts, answers_norm))

        # ê²°ê³¼ ì§‘ê³„
        recall_k = float(np.mean(hits)) if hits else 0.0
        avg_r = float(np.mean(r_times)) if r_times else 0.0
        min_r = float(np.min(r_times)) if r_times else 0.0
        max_r = float(np.max(r_times)) if r_times else 0.0

        with open(OUTPUT_CSV, "a", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                ret_type, chunk, top_k,
                f"{recall_k:.4f}",
                f"{avg_r:.6f}", f"{min_r:.6f}", f"{max_r:.6f}",
            ])

        print(f"    >> ì™„ë£Œ: Recall@{top_k}={recall_k:.4f} | Avg {avg_r:.6f}s")

    print(f"\n[ì™„ë£Œ] ê²°ê³¼ ì €ì¥: {OUTPUT_CSV}")

if __name__ == "__main__":
    run_retrieval_benchmarks()
