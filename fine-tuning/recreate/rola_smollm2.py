import os
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig
from trl import SFTTrainer, SFTConfig

# -----------------------------
# 1. í™˜ê²½ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
# -----------------------------
MAX_LEN = 1024
BASE_MODEL_DIR = os.path.expanduser("~/Desktop/llama.cpp/smollm2_135m_instruct")
TRAIN_JSONL = "data/rola_data/train_oracle_plus_idk_ver2.jsonl"
OUT_DIR = "outputs/smollm2_135m_rag_lora"

# -----------------------------
# 2. í…œí”Œë¦¿ ì¼ì¹˜í™” í•¨ìˆ˜ (NAIVE_RAG_PROMPTì™€ ë™ì¼ êµ¬ì¡°)
# -----------------------------
def build_text(ex):
    p = ex["prompt"].rstrip()
    r = ex["response"].rstrip()
    c = (ex.get("context") or "").rstrip()

    # ì¶”ë¡  í”„ë¡¬í”„íŠ¸ì™€ ì™„ë²½íˆ ì¼ì¹˜í•˜ëŠ” êµ¬ì¡°ë¡œ ìƒì„±
    if c:
        text = (
            "You are a careful assistant.\n\n"
            "Rules:\n"
            "- Use ONLY the provided context.\n"
            "- Do NOT use external knowledge or assumptions.\n"
            "- Answer concisely (1â€“3 sentences).\n"
            "- If the answer cannot be determined from the context, say exactly: \"I don't know\".\n\n"
            "Context:\n"
            f"{c}\n\n"
            "Question:\n"
            f"{p}\n\n"
            "Output format:\n"
            "Answer: <1â€“3 sentences>\n"
            "Evidence: [<sentence_ids>]\n\n"
            "Answer:\n" # ì¶”ë¡  ì‹œ ì…ë ¥ì´ ëë‚˜ëŠ” ì§€ì 
            f"{r}"       # ëª¨ë¸ì´ ìƒì„±í•´ì•¼ í•  ì •ë‹µ
        )
    else:
        # Contextê°€ ì—†ëŠ” ê²½ìš° (ONLY_SLM ëŒ€ì‘)
        text = (
            "Answer the following question.\n\n"
            "Rules:\n"
            "- Answer concisely (1â€“3 sentences).\n"
            "- Be factual.\n"
            "- If you do not know the answer, say exactly: \"I don't know\".\n\n"
            f"Question:\n{p}\n\n"
            "Answer:\n"
            f"{r}"
        )
    return {"text": text}

def main():
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tok = AutoTokenizer.from_pretrained(BASE_MODEL_DIR, use_fast=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token

    # ëª¨ë¸ ë¡œë“œ (dtype ì¸ì ì‚¬ìš©)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_DIR,
        device_map="auto",
        dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    )
    model.config.use_cache = False

    # ë°ì´í„° ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë³€í™˜
    ds = load_dataset("json", data_files=TRAIN_JSONL, split="train")
    ds = ds.map(build_text)

    # [ì¤‘ìš”] ë¯¸ë¦¬ í† í°í™”í•˜ì—¬ SFTConfig ì¸ì ì—ëŸ¬ ì›ì²œ ì°¨ë‹¨
    def tokenize_fn(batch):
        out = tok(
            batch["text"],
            truncation=True,
            max_length=MAX_LEN,
            padding=False,
        )
        out["labels"] = out["input_ids"].copy()
        return out

    # í† í°í™” í›„ ë¶ˆí•„ìš”í•œ ì›ë³¸ ì»¬ëŸ¼ ì œê±°
    ds = ds.map(tokenize_fn, batched=True, remove_columns=ds.column_names)

    # LoRA ì„¤ì • (ëª¨ë“  ì„ í˜• ë ˆì´ì–´ í•™ìŠµ)
    lora_cfg = LoraConfig(
        r=16, 
        lora_alpha=32, 
        lora_dropout=0.05,
        task_type="CAUSAL_LM",
        target_modules="all-linear",
        bias="none",
    )

    # í•™ìŠµ ì„¤ì • (ìˆœìˆ˜ í•™ìŠµ ê´€ë ¨ ì¸ìë§Œ ìœ ì§€)
    sft_cfg = SFTConfig(
        output_dir=OUT_DIR,
        per_device_train_batch_size=8,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        num_train_epochs=1,
        logging_steps=10,
        save_steps=100,
        bf16=torch.cuda.is_available(),
        report_to="none",
        eval_strategy="no",
    )

    # íŠ¸ë ˆì´ë„ˆ ì‹¤í–‰
    trainer = SFTTrainer(
        model=model,
        train_dataset=ds,
        peft_config=lora_cfg,
        args=sft_cfg,
        processing_class=tok,
    )

    print("--- ğŸš€ í•™ìŠµ ì‹œì‘ (í…œí”Œë¦¿ ì¼ì¹˜ ì™„ë£Œ) ğŸš€ ---")
    trainer.train()
    
    # ì €ì¥
    trainer.save_model(OUT_DIR)
    tok.save_pretrained(OUT_DIR)
    print(f"[OK] ëª¨ë¸ê³¼ ì–´ëŒ‘í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {OUT_DIR}")

if __name__ == "__main__":
    main()