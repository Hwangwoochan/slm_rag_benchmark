import os
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig
from trl import SFTTrainer, SFTConfig

# -----------------------------
# 1. í™˜ê²½ ì„¤ì • (ëª¨ë¸ ê²½ë¡œë§Œ Qwenìœ¼ë¡œ ë°”ê¾¸ì‹œë©´ ë©ë‹ˆë‹¤)
# -----------------------------
MAX_LEN = 1024
# Qwen2.5-0.5B-Instruct ë˜ëŠ” 1.5B ê²½ë¡œë¡œ ì„¤ì •í•˜ì„¸ìš”.
BASE_MODEL_DIR = os.path.expanduser("~/Desktop/models/Qwen2.5-0.5B-Instruct")
TRAIN_JSONL = "data/rola_data/train_oracle_plus_idk_ver2.jsonl"
OUT_DIR = "outputs/qwen2.5_0.5b_rag_lora"

# -----------------------------
# 2. í…œí”Œë¦¿ ì¼ì¹˜í™” (SmolLM2ì™€ 100% ë™ì¼)
# -----------------------------
def build_text(ex):
    p = ex["prompt"].rstrip()
    r = ex["response"].rstrip()
    c = (ex.get("context") or "").rstrip()

    if c:
        text = (
            "You are a careful assistant.\n\n"
            "Rules:\n"
            "- Use ONLY the provided context.\n"
            "- Do NOT use external knowledge or assumptions.\n"
            "- Answer concisely (1â€“3 sentences).\n"
            "- If the answer cannot be determined from the context, say exactly: \"I don't know\".\n\n"
            "Context:\n"
            f"{c}\n\n"
            "Question:\n"
            f"{p}\n\n"
            "Output format:\n"
            "Answer: <1â€“3 sentences>\n"
            "Evidence: [<sentence_ids>]\n\n"
            "Answer:\n" 
            f"{r}"
        )
    else:
        text = (
            "Answer the following question.\n\n"
            "Rules:\n"
            "- Answer concisely (1â€“3 sentences).\n"
            "- Be factual.\n"
            "- If you do not know the answer, say exactly: \"I don't know\".\n\n"
            f"Question:\n{p}\n\n"
            "Answer:\n"
            f"{r}"
        )
    return {"text": text}

def main():
    # í† í¬ë‚˜ì´ì € ë¡œë“œ (Qwenì€ trust_remote_code ê¶Œì¥)
    tok = AutoTokenizer.from_pretrained(BASE_MODEL_DIR, trust_remote_code=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token

    # ëª¨ë¸ ë¡œë“œ (Qwenì€ bfloat16 ì§€ì›ì´ ë§¤ìš° ì¢‹ìŠµë‹ˆë‹¤)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_DIR,
        device_map="auto",
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
        trust_remote_code=True
    )
    model.config.use_cache = False

    # ë°ì´í„°ì…‹ ì¤€ë¹„
    ds = load_dataset("json", data_files=TRAIN_JSONL, split="train")
    ds = ds.map(build_text)

    # ë¯¸ë¦¬ í† í°í™” (ë²„ì „ ì—ëŸ¬ ë°©ì§€ìš©)
    def tokenize_fn(batch):
        out = tok(batch["text"], truncation=True, max_length=MAX_LEN, padding=False)
        out["labels"] = out["input_ids"].copy()
        return out

    ds = ds.map(tokenize_fn, batched=True, remove_columns=ds.column_names)

    # 3. LoRA ì„¤ì • (Qwen ìµœì í™”)
    lora_cfg = LoraConfig(
        r=16, 
        lora_alpha=32, 
        lora_dropout=0.05,
        task_type="CAUSAL_LM",
        target_modules="all-linear", # Qwen2.5ì˜ ëª¨ë“  ì„ í˜• ë ˆì´ì–´ íƒ€ê²ŸíŒ…
        bias="none",
    )

    # 4. í•™ìŠµ ì„¤ì • (Qwenì€ ì¡°ê¸ˆ ë” ì •êµí•œ í•™ìŠµì´ í•„ìš”í•˜ì—¬ Epochê³¼ LR ì¡°ì •)
    sft_cfg = SFTConfig(
        output_dir=OUT_DIR,
        per_device_train_batch_size=4,   # ëª¨ë¸ì´ ì»¤ì¡Œìœ¼ë¯€ë¡œ 8ì—ì„œ 4ë¡œ ì¡°ì •
        gradient_accumulation_steps=8,  # ë°°ì¹˜ë¥¼ ë§ì¶”ê¸° ìœ„í•´ 4ì—ì„œ 8ë¡œ ì¡°ì •
        learning_rate=1e-4,             # Qwenì€ 1e-4ê°€ ë” ì•ˆì •ì ì…ë‹ˆë‹¤.
        num_train_epochs=3,             # RAG ê·œì¹™ì„ í™•ì‹¤íˆ ë°°ìš°ê¸° ìœ„í•´ 3 Epoch ê¶Œì¥
        logging_steps=10,
        save_steps=100,
        bf16=torch.cuda.is_available(),
        report_to="none",
        eval_strategy="no",
    )

    trainer = SFTTrainer(
        model=model,
        train_dataset=ds,
        peft_config=lora_cfg,
        args=sft_cfg,
        processing_class=tok,
    )

    print(f"--- ğŸš€ Qwen2.5 RAG í•™ìŠµ ì‹œì‘ (Target: {BASE_MODEL_DIR}) ---")
    trainer.train()
    
    trainer.save_model(OUT_DIR)
    tok.save_pretrained(OUT_DIR)
    print(f"[OK] ì €ì¥ ì™„ë£Œ: {OUT_DIR}")

if __name__ == "__main__":
    main()